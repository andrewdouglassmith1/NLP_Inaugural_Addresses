{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "scientific-arnold",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "essential-victorian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import pickle\n",
    "import spacy\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "danish-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Functions\n",
    "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hidden-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # convert sentence into token of words\n",
    "    all_addresses = []\n",
    "    for addresses in text:    \n",
    "        tokens = tokenizer.tokenize(addresses)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        # check in lowercase \n",
    "        t = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        text=' '.join(t)\n",
    "        all_addresses.append(text)    \n",
    "    return pd.Series(all_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "automated-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text,allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    # Create list to store all addresses in\n",
    "    all_addresses = []\n",
    "    for address in text:\n",
    "        doc = nlp(address)\n",
    "        t = \" \".join([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        all_addresses.append(t)\n",
    "    return pd.Series(all_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-quarterly",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intensive-rwanda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president_number</th>\n",
       "      <th>term</th>\n",
       "      <th>pres_name</th>\n",
       "      <th>pres_det</th>\n",
       "      <th>president_x</th>\n",
       "      <th>address</th>\n",
       "      <th>party</th>\n",
       "      <th>Year</th>\n",
       "      <th>time_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1 Washington</td>\n",
       "      <td>01.Washington.1.txt</td>\n",
       "      <td>AMONG the vicissitudes incident to life no eve...</td>\n",
       "      <td>Nonpartisan</td>\n",
       "      <td>1789</td>\n",
       "      <td>pre-1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   president_number term   pres_name      pres_det          president_x  \\\n",
       "0                 1    1  Washington  1 Washington  01.Washington.1.txt   \n",
       "\n",
       "                                             address        party  Year  \\\n",
       "0  AMONG the vicissitudes incident to life no eve...  Nonpartisan  1789   \n",
       "\n",
       "  time_period  \n",
       "0    pre-1800  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pickled Dataframe from Cleaning Notebook into a DataFrame\n",
    "path = r\"C:\\Users\\Andrew\\Documents\\Metis\\NLP_Inaugural_Addresses\\Pickled_Files\\cleaned_addresses.pkl\"\n",
    "\n",
    "df = pickle.load(open(path,'rb'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-furniture",
   "metadata": {},
   "source": [
    "## Define Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "casual-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['thing','year','ago','people','nation','states', 'make','long','come','day','know','day','way','fellow'\n",
    "               ,'americans','citizens','citizen','united','america','shall','must','may','upon','every','let','one','would','great']\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-trail",
   "metadata": {},
   "source": [
    "## Breakdown by Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "corresponding-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generous-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'] = df['address'].apply(lambda x: [sent.text for sent in nlp(x).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "insured-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df.explode(\"address\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "arranged-morgan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sustained by faith, driven by conviction and devoted to one another and the country we love with all our hearts.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences['address'][5842]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-termination",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "developed-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text array\n",
    "text = df_sentences['address']\n",
    "\n",
    "# Remove stopwords\n",
    "text = remove_stopwords(text)\n",
    "\n",
    "# Lemmatize\n",
    "text = lemma(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "negative-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer and fit to text\n",
    "vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\\b[^\\d\\W]+\\b',stop_words = stopwords,binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fabulous-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = vectorizer.fit_transform(text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "brief-position",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: war,invasion,prevent,state,control,part,permit,product,organization,partial\n",
      "1: peace,world,promote,commerce,force,defense,maintain,international,army,military\n",
      "2: interest,foreign,power,country,proper,policy,opinion,domestic,necessary,object\n",
      "3: economy,public,government,expenditure,exercise,political,enable,health,business,sufficient\n",
      "4: freedom,man,liberty,hope,human,life,woman,love,happiness,dignity\n",
      "5: equal,duty,right,law,high,good,discharge,protection,protect,give\n",
      "6: uphold,home,abroad,child,farm,factory,destruction,leave,old,spiritual\n",
      "7: revenue,executive,tax,taxation,branch,tariff,department,legislative,money,system\n",
      "8: preserve,oath,constitution,office,confidence,take,countryman,principle,express,measure\n"
     ]
    }
   ],
   "source": [
    "topic_model = ct.Corex(n_hidden=9, words=words,\n",
    "                       max_iter=200, verbose=False, seed=1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=text, \n",
    "                anchors=[['war','invasion'], \n",
    "                         ['peace'], \n",
    "                         ['foreign','interest'], \n",
    "                         ['economy'],\n",
    "                         ['freedom'],\n",
    "                         ['equal'],\n",
    "                         ['uphold'],\n",
    "                         ['taxation','tax'],\n",
    "                        ['constitution','preserve']], anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "static-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('firm reliance goodness providence mercifully protect national infancy uphold libertie various vicissitude encourage offer ardent supplication continue make beloved country object divine care gracious benediction',\n",
       "  0.0),\n",
       " ('do justice occasion favor favor lawful cherished mutual interest intercourse fair equal term',\n",
       "  0.0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out topic : graphics\n",
    "topic_model.get_top_docs(topic=5, n_docs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

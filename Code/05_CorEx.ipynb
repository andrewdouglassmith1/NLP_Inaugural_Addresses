{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "controlling-soundtrack",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "elect-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import datasets\n",
    "\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import pickle\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "amateur-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Functions\n",
    "nltk.download('stopwords', quiet=True, raise_on_error=True)\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english'))\n",
    "tokenized_stop_words = nltk.word_tokenize(' '.join(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "stopped-biography",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # convert sentence into token of words\n",
    "    all_addresses = []\n",
    "    for addresses in text:    \n",
    "        tokens = tokenizer.tokenize(addresses)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        # check in lowercase \n",
    "        t = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        text=' '.join(t)\n",
    "        all_addresses.append(text)    \n",
    "    return pd.Series(all_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "liberal-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(text,allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "    # Create list to store all addresses in\n",
    "    all_addresses = []\n",
    "    for address in text:\n",
    "        doc = nlp(address)\n",
    "        t = \" \".join([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        all_addresses.append(t)\n",
    "    return pd.Series(all_addresses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-biology",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "organic-ivory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>president_number</th>\n",
       "      <th>term</th>\n",
       "      <th>pres_name</th>\n",
       "      <th>pres_det</th>\n",
       "      <th>president_x</th>\n",
       "      <th>address</th>\n",
       "      <th>party</th>\n",
       "      <th>Year</th>\n",
       "      <th>time_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1 Washington</td>\n",
       "      <td>01.Washington.1.txt</td>\n",
       "      <td>AMONG the vicissitudes incident to life no eve...</td>\n",
       "      <td>Nonpartisan</td>\n",
       "      <td>1789</td>\n",
       "      <td>pre-1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   president_number term   pres_name      pres_det          president_x  \\\n",
       "0                 1    1  Washington  1 Washington  01.Washington.1.txt   \n",
       "\n",
       "                                             address        party  Year  \\\n",
       "0  AMONG the vicissitudes incident to life no eve...  Nonpartisan  1789   \n",
       "\n",
       "  time_period  \n",
       "0    pre-1800  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Pickled Dataframe from Cleaning Notebook into a DataFrame\n",
    "path = r\"C:\\Users\\Andrew\\Documents\\Metis\\NLP_Inaugural_Addresses\\Pickled_Files\\cleaned_addresses.pkl\"\n",
    "\n",
    "df = pickle.load(open(path,'rb'))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-blast",
   "metadata": {},
   "source": [
    "## Define Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "selective-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add additional stop words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['thing','year','ago','people','nation','states', 'make','long','come','day','know','day','way','fellow'\n",
    "               ,'americans','citizens','citizen','united','america','shall','must','may','upon','every','let','one','would','great']\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-timeline",
   "metadata": {},
   "source": [
    "## Breakdown by Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "suited-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "polyphonic-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['address'] = df['address'].apply(lambda x: [sent.text for sent in nlp(x).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "surprised-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df.explode(\"address\", ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "stock-republican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sustained by faith, driven by conviction and devoted to one another and the country we love with all our hearts.'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences['address'][5842]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-research",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "latest-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text array\n",
    "text = df_sentences['address']\n",
    "\n",
    "# Remove stopwords\n",
    "text = remove_stopwords(text)\n",
    "\n",
    "# Lemmatize\n",
    "text = lemma(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "raised-leave",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorizer and fit to text\n",
    "vectorizer = CountVectorizer(analyzer='word',token_pattern=r'\\b[^\\d\\W]+\\b',stop_words = stopwords,binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "stuffed-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word = vectorizer.fit_transform(text)\n",
    "words = list(np.asarray(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "measured-cooking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: war,invasion,manufacture,internal,improvement,patriot,city,destruction,experience,effect\n",
      "1: peace,world,establish,commerce,maintain,promote,international,harmony,force,contribute\n",
      "2: interest,foreign,power,proper,country,executive,revenue,necessary,object,state\n",
      "3: public,economy,law,duty,expenditure,support,administration,business,system,execute\n",
      "4: freedom,man,life,human,woman,young,history,new,opportunity,mankind\n",
      "5: government,political,right,equality,form,principle,opinion,institution,exercise,settle\n",
      "6: preserve,oath,constitution,liberty,blessing,high,office,take,honor,uphold\n"
     ]
    }
   ],
   "source": [
    "topic_model = ct.Corex(n_hidden=7, words=words,\n",
    "                       max_iter=200, verbose=False, seed=1)\n",
    "\n",
    "topic_model.fit(doc_word, words=words, docs=text, \n",
    "                anchors=[['war','invasion'], \n",
    "                         ['peace'], \n",
    "                         ['foreign','interest'], \n",
    "                         ['economy'],\n",
    "                         ['freedom'],\n",
    "                         ['equality'],\n",
    "                        ['constitution','preserve','uphold']], anchor_strength=2)\n",
    "\n",
    "# Print all topics from the CorEx topic model\n",
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "independent-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('therefore thing stand war peace nation equally interested peace world political stability free people equally responsible maintenance essential principle peace actual equality nation matter right privilege peace securely justly rest armed balance power government derive power consent govern power support common thought purpose power family nation sea equally free safe use people rule set common agreement consent far practicable accessible equal term national armament limited necessity national order domestic safety community interest power peace henceforth depend impose nation duty seeing influence proceed citizen mean encourage assist revolution state sternly effectually suppress prevent',\n",
       "  0.0),\n",
       " ('declare part long make reality name argument postulation exhaust positive declaration receive wrong provoking discontinue last appeal long delay break spirit nation destroy confidence political institution perpetuate state disgraceful suffering regain costly sacrifice severe struggle lose rank respect independent power',\n",
       "  0.0)]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out topic : graphics\n",
    "topic_model.get_top_docs(topic=5, n_docs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
